{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e9a17c8-3385-4dbf-9538-525b62221409",
   "metadata": {},
   "source": [
    "Consider different types of group assignments, and measure the within-group consistency (pairwise distances).\n",
    "\n",
    "Start with the one currently implemented in `group_matching.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d0e576b-3b0f-4a40-ba79-b7b2507db94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Google ortools library for ILP solver.\n",
      "Number of registered users: 1162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickmineault/miniconda3/envs/py3/lib/python3.9/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
      "1162it [02:56,  6.59it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "import hcluster   # requires dedupe-hcluster\n",
    "from paper_reviewer_matcher import (\n",
    "    preprocess, compute_affinity\n",
    ")\n",
    "\n",
    "from group_matching import compute_conflicts, generate_pod_numbers\n",
    "\n",
    "users = pd.read_csv('data/mindmatch_example.csv').to_dict(orient='records')\n",
    "n_users = len(users)\n",
    "print('Number of registered users: {}'.format(n_users))\n",
    "\n",
    "users_df = pd.DataFrame(users).fillna('')\n",
    "users_dict = {r['user_id']: dict(r) for _, r in users_df.iterrows()}  # map of user id to details\n",
    "persons_1 = list(map(preprocess, list(users_df['abstracts'])))\n",
    "persons_2 = list(map(preprocess, list(users_df['abstracts'])))\n",
    "A = compute_affinity(\n",
    "    persons_1, persons_2,\n",
    "    n_components=30, min_df=2, max_df=0.8,\n",
    "    weighting='tfidf', projection='svd'\n",
    ")\n",
    "cois_list = compute_conflicts(users_df)\n",
    "for i, j in cois_list:\n",
    "    A[i, j] = -1\n",
    "\n",
    "A_cluster = - A\n",
    "A_cluster[A_cluster == 1000] = 1\n",
    "A_rand = np.random.randn(n_users, n_users) * 0.01 * A_cluster.var() # add randomness\n",
    "\n",
    "z = linkage(A_cluster + A_rand,\n",
    "            method='average',\n",
    "            metric='euclidean',\n",
    "            optimal_ordering=True)\n",
    "cluster = hcluster.fcluster(z, t=0.01,\n",
    "                            criterion='distance') # distance\n",
    "\n",
    "# Measure the goodness of the assignment\n",
    "def generate_pod_numbers(n_users, n_per_group):\n",
    "    \"\"\"\n",
    "    Generate pod numbers in sequence\n",
    "    \"\"\"\n",
    "    ngroups = int(np.ceil(A.shape[0] / n_per_group))\n",
    "    nsmallgroups = ngroups * n_per_group - A.shape[0]\n",
    "    nbiggroups = ngroups - nsmallgroups\n",
    "    \n",
    "    group_sizes = [n_per_group] * nbiggroups + [n_per_group - 1] * nsmallgroups\n",
    "    groups = []\n",
    "    for i, gs in enumerate(group_sizes):\n",
    "        groups.extend([i] * gs)\n",
    "    return groups\n",
    "\n",
    "cluster_numbers = generate_pod_numbers(n_users=A.shape[0], n_per_group=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c55fbb3-137e-4a47-877f-9c19eb7f5e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conventional method\n",
      "[-0.23604346738459883, 0.06337616032926523]\n",
      "Random assignment\n",
      "[-0.3248631461787265, 0.04439390372145903]\n"
     ]
    }
   ],
   "source": [
    "def measure_goodness(A_cluster, cluster_assignments):\n",
    "    dists = []\n",
    "    for i in range(cluster_assignments.min(), cluster_assignments.max()+1):\n",
    "        # Calculate the average pairwise distance within the cluster.\n",
    "        mean_dist = A_cluster[cluster_assignments == i, :][:, cluster_assignments == i].mean()\n",
    "        dists.append(mean_dist)\n",
    "        \n",
    "    return dists\n",
    "\n",
    "assignments = sorted([(id_, cluster_numbers[cluster]) for cluster, id_ in sorted(zip(cluster, np.arange(A.shape[0])))])\n",
    "assignments = np.array([y for x, y in assignments])\n",
    "\n",
    "# Measure the conventional method\n",
    "print(\"Conventional method\")\n",
    "goodnesses = np.array(measure_goodness(A, assignments))\n",
    "print([goodnesses.mean(), np.std(goodnesses)])\n",
    "\n",
    "print(\"Random assignment\")\n",
    "goodnesses = np.array(measure_goodness(A, np.array(cluster_numbers)))\n",
    "print([goodnesses.mean(), np.std(goodnesses)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1c2b9f-7ccd-4dbb-8333-ae790ffa7e76",
   "metadata": {},
   "source": [
    "Now implement an alternative method based off of multi-pass hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52626f09-2e63-41e5-946a-4f36cd59a722",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 233/233 [00:14<00:00, 16.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bottom up\n",
      "[-0.22207694200864384, 0.057677851594478996]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# We apply the alternative bottom-up method suggested here\n",
    "# https://github.com/jmonlong/Hippocamplus/blob/master/content/post/2018-06-09-ClusterEqualSize.Rmd\n",
    "def agglomerate(A, group_size):\n",
    "    ngroups = int(np.ceil(A.shape[0] / group_size))\n",
    "    nsmallgroups = ngroups * group_size - A.shape[0]\n",
    "    nbiggroups = ngroups - nsmallgroups\n",
    "    labels = np.ones(A.shape[0]) * np.nan\n",
    "\n",
    "    A0 = A.copy()\n",
    "    \n",
    "    groups = []\n",
    "    group_sizes = [group_size] * nbiggroups + [group_size - 1] * nsmallgroups\n",
    "    assert A.shape[0] == sum(group_sizes)\n",
    "    j = 0\n",
    "    for gs in tqdm(group_sizes):\n",
    "        B = A[np.isnan(labels), :][:, np.isnan(labels)]\n",
    "        z = linkage(B,\n",
    "                    method='average',\n",
    "                    metric='euclidean')\n",
    "        \n",
    "        the_nums = np.where(z[:, -1] >= gs)[0]\n",
    "        if the_nums.size == 0:\n",
    "            print(the_nums)\n",
    "            print(z)\n",
    "            print(B.shape)\n",
    "        minpos = the_nums.min()\n",
    "        \n",
    "        cluster_nums = [z[minpos, 0], z[minpos, 1]]\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(cluster_nums):\n",
    "            if cluster_nums[i] >= B.shape[0]:\n",
    "                cluster_nums.append(z[int(cluster_nums[i]) - B.shape[0], 0])\n",
    "                cluster_nums.append(z[int(cluster_nums[i]) - B.shape[0], 1])\n",
    "            i += 1\n",
    "            \n",
    "        cluster_nums = np.array(cluster_nums).astype(int)\n",
    "        cluster_nums = cluster_nums[cluster_nums < B.shape[0]]\n",
    "        \n",
    "        assert len(cluster_nums) >= gs\n",
    "        cluster_nums = cluster_nums[:gs]\n",
    "        \n",
    "        # Map cluster nums to the original numbers prior to subsetting.\n",
    "        the_map = np.where(np.isnan(labels))[0]        \n",
    "        cluster_nums = [the_map[k] for k in cluster_nums]\n",
    "        labels[cluster_nums] = j        \n",
    "        j += 1\n",
    "        \n",
    "    return labels.astype(int)\n",
    "\n",
    "labels = agglomerate(A_cluster + A_rand, 5)\n",
    "\n",
    "print(\"Bottom up\")\n",
    "goodnesses = np.array(measure_goodness(A, labels))\n",
    "print([goodnesses.mean(), np.std(goodnesses)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e0fc9a-76e3-4479-86fc-fd51f163d70f",
   "metadata": {},
   "source": [
    "The second method does a little bit better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
